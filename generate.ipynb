{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7429,)\n",
      "23674\n",
      "7429\n",
      "\n",
      "Training dataset : <BatchDataset shapes: ((1, 256), (1, 256, 296)), types: (tf.int64, tf.float32)>\n",
      "\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (1, None, 256)            75776     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (1, None, 512)            1574912   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (1, None, 512)            2048      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1, None, 512)            2099200   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (1, None, 512)            2048      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (1, None, 512)            2099200   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (1, None, 512)            2048      \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (1, None, 512)            2099200   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (1, None, 512)            2048      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (1, None, 512)            262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (1, None, 512)            2048      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (1, None, 512)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 296)            151848    \n",
      "=================================================================\n",
      "Total params: 8,373,032\n",
      "Trainable params: 8,367,912\n",
      "Non-trainable params: 5,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from dataloader import *\n",
    "from model import *\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "\n",
    "config = yaml.safe_load(open('config.yaml', 'r'))\n",
    "# batch_size = config['params']['batch_size']\n",
    "batch_size = 1\n",
    "\n",
    "train_dataset_path = \"data/train_dataset\"\n",
    "test_dataset_path = \"data/test_dataset\"\n",
    "\n",
    "train_dataset = Dataset(config['path']['data_dir'])\n",
    "if os.path.exists(train_dataset_path):\n",
    "    with open(train_dataset_path, \"rb\") as f:\n",
    "        train_dataset = pickle.load(f)\n",
    "else:\n",
    "    train_dataset.load()\n",
    "    with open(train_dataset_path, \"wb\") as f:\n",
    "        pickle.dump(train_dataset, f)\n",
    "\n",
    "\n",
    "test_dataset = Dataset(config['path']['test_dir'])\n",
    "if os.path.exists(test_dataset_path):\n",
    "    with open(test_dataset_path, \"rb\") as f:\n",
    "        test_dataset = pickle.load(f)\n",
    "else:\n",
    "    test_dataset.load()\n",
    "    with open(test_dataset_path, \"wb\") as f:\n",
    "        pickle.dump(test_dataset, f)\n",
    "\n",
    "beat_list = []\n",
    "track = 0\n",
    "for midi in train_dataset.encoded_data_list:\n",
    "    beat_list += midi[track]\n",
    "train_data = beat_list.copy()\n",
    "\n",
    "beat_list = []\n",
    "for midi in test_dataset.encoded_data_list:\n",
    "    beat_list += midi[track]\n",
    "test_data = beat_list.copy()\n",
    "\n",
    "mySet = set(train_data) | set(test_data)\n",
    "vocab_size = len(mySet)\n",
    "print(np.array(beat_list).shape)\n",
    "\n",
    "beat2idx = {beat: i for i, beat in enumerate(mySet)}\n",
    "idx2beat = {idx:char for char,idx in beat2idx.items()}\n",
    "\n",
    "def __split_input_target(chunk):\n",
    "    global vocab_size\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = tf.one_hot(chunk[1:], vocab_size)\n",
    "    return input_text, target_text\n",
    "\n",
    "\n",
    "idxOfBeat = np.array([beat2idx[beat] for beat in train_data])\n",
    "print(len(idxOfBeat))\n",
    "tfDataset = tf.data.Dataset.from_tensor_slices(idxOfBeat)\n",
    "sequences = tfDataset.batch(config['params']['sequence_length']+1, drop_remainder=True)\n",
    "# o o o o o\n",
    "#  \\ \\ \\ \\ \\\n",
    "#   x x x x x\n",
    "ds = sequences.map(__split_input_target)\n",
    "train_data = ds.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "\n",
    "idxOfBeat = np.array([beat2idx[beat] for beat in test_data])\n",
    "print(len(idxOfBeat))\n",
    "tfDataset = tf.data.Dataset.from_tensor_slices(idxOfBeat)\n",
    "sequences = tfDataset.batch(config['params']['sequence_length']+1, drop_remainder=True)\n",
    "# o o o o o\n",
    "#  \\ \\ \\ \\ \\\n",
    "#   x x x x x\n",
    "ds = sequences.map(__split_input_target)\n",
    "test_data = ds.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "print(f'\\nTraining dataset : {train_data}\\n')\n",
    "\n",
    "\n",
    "model = LoFiLoopNet('dev', vocab_size, batch_size=1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weight models/model-stateful-1855-1.5676-bigger.hdf5\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('models/model-stateful-1855-1.5676-bigger.hdf5')\n",
    "model.model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = test_data.enumerate()\n",
    "init_x = None\n",
    "init_y = None\n",
    "for index, (x, y) in td.as_numpy_iterator():\n",
    "    init_x = tf.expand_dims([x[0, 0]], 0)\n",
    "    init_y = tf.expand_dims([y[0, 0]], 0)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[77]], dtype=int32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.reset_states()\n",
    "pred = loaded_model(init_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_sequence = []\n",
    "\n",
    "for i in range(100):\n",
    "    prediction = tf.squeeze(pred, 0).numpy()\n",
    "    top_k = tf.math.top_k(prediction, 3)\n",
    "    top_k_choices = top_k[1].numpy().squeeze()\n",
    "    top_k_values = top_k[0].numpy().squeeze()\n",
    "\n",
    "    # Apply random\n",
    "    next_beat = None\n",
    "    if np.random.uniform(0, 1) < .5:\n",
    "        next_beat = top_k_choices[0]\n",
    "    else:\n",
    "        p_choices = tf.math.softmax(top_k_values[1:]).numpy()\n",
    "        next_beat = np.random.choice(top_k_choices[1:], 1, p=p_choices)[0]\n",
    "\n",
    "    music_sequence.append(idx2beat[next_beat])\n",
    "\n",
    "    expanded_input = tf.expand_dims([next_beat], 0)\n",
    "    loaded_model(expanded_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_midi = MidiData()\n",
    "new_midi.decode(music_sequence, filename='stateful-2-l1.5.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 ('lofi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af42d6b00596c858df57cfb1e3d56019ad36764fdda4c39cb12fa72d6aaee7c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
